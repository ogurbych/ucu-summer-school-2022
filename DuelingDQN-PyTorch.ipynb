{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMG8qMK9OY6r"
   },
   "source": [
    "# Install Dependecies to Render OpenAI Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvvBAoQVJsuU"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt-get update\n",
    "!pip install pyglet==1.3.2\n",
    "!pip install gym[atari] pyvirtualdisplay\n",
    "!apt-get install -y xvfb python-opengl ffmpeg\n",
    "!pip install box2d-py\n",
    "!pip install gast==0.2.2\n",
    "!pip install torch\n",
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor, AtariPreprocessing, FrameStack\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import io\n",
    "import math\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96qFDgjNwFtf"
   },
   "source": [
    "#### Check that there is a GPU avaiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "g_FHpZV7vMou",
    "outputId": "2eeaca31-8018-42e3-f244-8f456e4fd848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ob-AUEpDO8MR"
   },
   "source": [
    "# Start Environment and Build Dueling DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "rxOHU5VsU7JB",
    "outputId": "6c42b3f2-7ddd-41ed-dec2-e6248c27afe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of state features: (4, 84, 84)\n",
      "Number of possible actions: 6\n"
     ]
    }
   ],
   "source": [
    "# Load gym environment and get action and state spaces\n",
    "env = gym.make('PongNoFrameskip-v4')\n",
    "env = AtariPreprocessing(env,\n",
    "                         grayscale_obs=True,\n",
    "                         scale_obs=False,\n",
    "                         terminal_on_life_loss=True)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "\n",
    "num_state_feats = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "max_observation_values = env.observation_space.high\n",
    "\n",
    "print('Number of state features: {}'.format(num_state_feats))\n",
    "print('Number of possible actions: {}'.format(num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlPlv9PNQM61"
   },
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    \"\"\"Convolutional neural network for the Atari games.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_actions):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        std = math.sqrt(2.0 / (4 * 84 * 84))\n",
    "        nn.init.normal_(self.conv1.weight, mean=0.0, std=std)\n",
    "        self.conv1.bias.data.fill_(0.0)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        std = math.sqrt(2.0 / (32 * 4 * 8 * 8))\n",
    "        nn.init.normal_(self.conv2.weight, mean=0.0, std=std)\n",
    "        self.conv2.bias.data.fill_(0.0)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        std = math.sqrt(2.0 / (64 * 32 * 4 * 4))\n",
    "        nn.init.normal_(self.conv3.weight, mean=0.0, std=std)\n",
    "        self.conv3.bias.data.fill_(0.0)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        std = math.sqrt(2.0 / (64 * 64 * 3 * 3))\n",
    "        nn.init.normal_(self.fc1.weight, mean=0.0, std=std)\n",
    "        self.fc1.bias.data.fill_(0.0)\n",
    "        self.V = nn.Linear(512, 1)\n",
    "        self.A = nn.Linear(512, num_actions)\n",
    "\n",
    "\n",
    "    def forward(self, states):\n",
    "        \"\"\"Forward pass of the neural network with some inputs.\"\"\"\n",
    "        x = F.relu(self.conv1(states))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1)))  # Flatten imathut.\n",
    "        V = self.V(x)\n",
    "        A = self.A(x)\n",
    "        Q = V + (A - A.mean(dim=1, keepdim=True))\n",
    "        return Q\n",
    "\n",
    "    \n",
    "# Create main and target neural networks.\n",
    "main_nn = DuelingDQN(num_actions).to(device)\n",
    "target_nn = DuelingDQN(num_actions).to(device)\n",
    "\n",
    "# Loss function and optimizer.\n",
    "optimizer = torch.optim.Adam(main_nn.parameters(), lr=1e-5)\n",
    "loss_fn = nn.SmoothL1Loss()  # Huber loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSbGFEG7IZit"
   },
   "source": [
    "# Create Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1W09AvQhrXfL"
   },
   "outputs": [],
   "source": [
    "def select_epsilon_greedy_action(state, epsilon):\n",
    "    \"\"\"Take random action with probability epsilon, else take best action.\"\"\"\n",
    "    \n",
    "    result = np.random.uniform()\n",
    "    \n",
    "    if result < epsilon:\n",
    "        return env.action_space.sample() # Random action.\n",
    "    else:\n",
    "        qs = main_nn(state).cpu().data.numpy()\n",
    "        return np.argmax(qs) # Greedy action for state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CD8-Ecn0ZJdE"
   },
   "outputs": [],
   "source": [
    "class UniformBuffer(object):\n",
    "    \"\"\"Experience replay buffer that samples uniformly.\"\"\"\n",
    "\n",
    "    def __init__(self, size, device):\n",
    "        self._size = size\n",
    "        self.buffer = []\n",
    "        self.device = device\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        if self._next_idx >= len(self.buffer):\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self._next_idx] = (state, action, reward, next_state, done)\n",
    "        self._next_idx = (self._next_idx + 1) % self._size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        idx = np.random.choice(len(self.buffer), num_samples)\n",
    "        \n",
    "        for i in idx:\n",
    "            elem = self.buffer[i]\n",
    "            state, action, reward, next_state, done = elem\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(done)\n",
    "        \n",
    "        states = torch.as_tensor(np.array(states), device=self.device)\n",
    "        actions = torch.as_tensor(np.array(actions), device=self.device)\n",
    "        rewards = torch.as_tensor(np.array(rewards, dtype=np.float32),\n",
    "                                  device=self.device)\n",
    "        next_states = torch.as_tensor(np.array(next_states), device=self.device)\n",
    "        dones = torch.as_tensor(np.array(dones, dtype=np.float32),\n",
    "                                device=self.device)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDaqKF-iH5p9"
   },
   "source": [
    "# Set Up Function to Perform a Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-jC9-cgWPyu"
   },
   "outputs": [],
   "source": [
    "def train_step(states, actions, rewards, next_states, dones):\n",
    "    \"\"\"Perform a training iteration on a batch of data.\"\"\"\n",
    "    \n",
    "    next_qs_argmax = main_nn(next_states).argmax(dim=-1, keepdim=True)\n",
    "    masked_next_qs = target_nn(next_states).gather(1, next_qs_argmax).squeeze()\n",
    "    target = rewards + (1.0 - dones) * discount * masked_next_qs\n",
    "    masked_qs = main_nn(states).gather(1, actions.unsqueeze(dim=-1)).squeeze()\n",
    "    loss = loss_fn(masked_qs, target.detach())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dk4gbIkIAkq9"
   },
   "source": [
    "# Start running the DQN algorithm and see how the algorithm learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnQ-tPIp9L2x"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters.\n",
    "num_episodes = 1200 # @param {type:\"integer\"}\n",
    "epsilon = 1.0 # @param {type:\"number\"}\n",
    "batch_size = 32 # @param {type:\"integer\"}\n",
    "discount = 0.99 # @param {type:\"number\"}\n",
    "buffer_size = 200000 # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "id": "psFPuBeiXED5",
    "outputId": "c26e9e9d-5209-4717-bd0b-a018eca894f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0/1200, Epsilon: 0.999, Loss: 0.0153, Return: -19.00\n",
      "Episode: 25/1200, Epsilon: 0.974, Loss: 0.0003, Return: -20.46\n",
      "Episode: 50/1200, Epsilon: 0.950, Loss: 0.0016, Return: -20.45\n",
      "Episode: 75/1200, Epsilon: 0.925, Loss: 0.0001, Return: -20.41\n",
      "Episode: 100/1200, Epsilon: 0.900, Loss: 0.0009, Return: -20.39\n",
      "Episode: 125/1200, Epsilon: 0.874, Loss: 0.0014, Return: -20.28\n",
      "Episode: 150/1200, Epsilon: 0.849, Loss: 0.0038, Return: -20.26\n",
      "Episode: 175/1200, Epsilon: 0.822, Loss: 0.0022, Return: -20.21\n",
      "Episode: 200/1200, Epsilon: 0.796, Loss: 0.0025, Return: -20.13\n",
      "Episode: 225/1200, Epsilon: 0.770, Loss: 0.0018, Return: -20.13\n",
      "Episode: 250/1200, Epsilon: 0.743, Loss: 0.0030, Return: -20.06\n",
      "Episode: 275/1200, Epsilon: 0.717, Loss: 0.0029, Return: -19.99\n",
      "Episode: 300/1200, Epsilon: 0.689, Loss: 0.0021, Return: -19.97\n",
      "Episode: 325/1200, Epsilon: 0.662, Loss: 0.0076, Return: -20.01\n",
      "Episode: 350/1200, Epsilon: 0.634, Loss: 0.0030, Return: -19.97\n",
      "Episode: 375/1200, Epsilon: 0.604, Loss: 0.0013, Return: -19.98\n",
      "Episode: 400/1200, Epsilon: 0.572, Loss: 0.0112, Return: -19.80\n",
      "Episode: 425/1200, Epsilon: 0.537, Loss: 0.0035, Return: -19.38\n",
      "Episode: 450/1200, Epsilon: 0.499, Loss: 0.0020, Return: -19.20\n",
      "Episode: 475/1200, Epsilon: 0.458, Loss: 0.0039, Return: -18.63\n",
      "Episode: 500/1200, Epsilon: 0.418, Loss: 0.0032, Return: -18.38\n",
      "Episode: 525/1200, Epsilon: 0.377, Loss: 0.0018, Return: -18.42\n",
      "Episode: 550/1200, Epsilon: 0.332, Loss: 0.0069, Return: -18.12\n",
      "Episode: 575/1200, Epsilon: 0.283, Loss: 0.0030, Return: -18.19\n",
      "Episode: 600/1200, Epsilon: 0.228, Loss: 0.0031, Return: -18.01\n",
      "Episode: 625/1200, Epsilon: 0.167, Loss: 0.0033, Return: -17.54\n",
      "Episode: 650/1200, Epsilon: 0.100, Loss: 0.0019, Return: -17.26\n",
      "Episode: 675/1200, Epsilon: 0.023, Loss: 0.0018, Return: -16.92\n",
      "Episode: 700/1200, Epsilon: 0.010, Loss: 0.0022, Return: -16.46\n",
      "Episode: 725/1200, Epsilon: 0.010, Loss: 0.0036, Return: -16.06\n",
      "Episode: 750/1200, Epsilon: 0.010, Loss: 0.0006, Return: -15.50\n",
      "Episode: 775/1200, Epsilon: 0.010, Loss: 0.0060, Return: -14.52\n",
      "Episode: 800/1200, Epsilon: 0.010, Loss: 0.0009, Return: -14.33\n",
      "Episode: 825/1200, Epsilon: 0.010, Loss: 0.0015, Return: -13.62\n",
      "Episode: 850/1200, Epsilon: 0.010, Loss: 0.0024, Return: -12.19\n",
      "Episode: 875/1200, Epsilon: 0.010, Loss: 0.0006, Return: -10.89\n",
      "Episode: 900/1200, Epsilon: 0.010, Loss: 0.0013, Return: -8.63\n",
      "Episode: 925/1200, Epsilon: 0.010, Loss: 0.0015, Return: -5.45\n",
      "Episode: 950/1200, Epsilon: 0.010, Loss: 0.0029, Return: -2.06\n",
      "Episode: 975/1200, Epsilon: 0.010, Loss: 0.0010, Return: 1.39\n",
      "Episode: 1000/1200, Epsilon: 0.010, Loss: 0.0008, Return: 5.23\n",
      "Episode: 1025/1200, Epsilon: 0.010, Loss: 0.0037, Return: 7.26\n",
      "Episode: 1050/1200, Epsilon: 0.010, Loss: 0.0023, Return: 8.75\n",
      "Episode: 1075/1200, Epsilon: 0.010, Loss: 0.0011, Return: 10.57\n",
      "Episode: 1100/1200, Epsilon: 0.010, Loss: 0.0022, Return: 12.59\n",
      "Episode: 1125/1200, Epsilon: 0.010, Loss: 0.0004, Return: 15.28\n",
      "Episode: 1150/1200, Epsilon: 0.010, Loss: 0.0012, Return: 17.33\n",
      "Episode: 1175/1200, Epsilon: 0.010, Loss: 0.0006, Return: 18.45\n",
      "Episode: 1200/1200, Epsilon: 0.010, Loss: 0.0004, Return: 18.84\n"
     ]
    }
   ],
   "source": [
    "buffer = UniformBuffer(size=buffer_size, device=device)\n",
    "\n",
    "# Start training. Play game once and then train with a batch.\n",
    "last_100_ep_rewards, cur_frame = [], 0\n",
    "for episode in range(num_episodes+1):\n",
    "    state = env.reset()\n",
    "    ep_reward, done = 0, False\n",
    "    \n",
    "    while not done:\n",
    "        state_np = np.array(state, dtype=np.float32)\n",
    "        state_in = torch.as_tensor(np.expand_dims(state_np / 255., axis=0),\n",
    "                                   device=device)\n",
    "        action = select_epsilon_greedy_action(state_in, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        ep_reward += reward\n",
    "        reward = np.sign(reward)\n",
    "\n",
    "        # Save to experience replay.\n",
    "        buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        cur_frame += 1\n",
    "        \n",
    "        if epsilon > 0.01:\n",
    "            epsilon -= 1.1e-6\n",
    "\n",
    "        if len(buffer) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "            states = states.type(torch.FloatTensor).to(device) / 255.\n",
    "            next_states = next_states.type(torch.FloatTensor).to(device) / 255.\n",
    "            loss = train_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "    # Copy main_nn weights to target_nn.\n",
    "    if cur_frame % 10000 == 0:\n",
    "        target_nn.load_state_dict(main_nn.state_dict())\n",
    "\n",
    "    if len(last_100_ep_rewards) == 100:\n",
    "        last_100_ep_rewards = last_100_ep_rewards[1:]\n",
    "        \n",
    "    last_100_ep_rewards.append(ep_reward)\n",
    "\n",
    "    if episode % 25 == 0:\n",
    "        print(f'Episode: {episode}/{num_episodes}, Epsilon: {epsilon:.3f}, '\\\n",
    "              f'Loss: {loss:.4f}, Return: {np.mean(last_100_ep_rewards):.2f}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myngzgt3H_ch"
   },
   "source": [
    "## Helper functions to visualize the performance of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8sEgVgvM2OQ"
   },
   "outputs": [],
   "source": [
    "def show_video():\n",
    "    \"\"\"Enables video recording of gym environment and shows it.\"\"\"\n",
    "    \n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    \n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "    \n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                                        loop controls style=\"height: 400px;\">\n",
    "                                        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                                        </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Video not found\")\n",
    "\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxzxGYh_OmQg"
   },
   "source": [
    "# Display Result of Trained DQN Agent on Pong Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "2XelFhSJGWGX",
    "outputId": "e07be901-f96c-4418-c8c7-65b9272d2180"
   },
   "outputs": [],
   "source": [
    "env = gym.make('PongNoFrameskip-v4')\n",
    "env = AtariPreprocessing(env,\n",
    "                         grayscale_obs=True,\n",
    "                         scale_obs=True,\n",
    "                         terminal_on_life_loss=False)\n",
    "env = wrap_env(FrameStack(env, num_stack=4))\n",
    "\n",
    "state = env.reset()\n",
    "ep_rew, done = 0, False\n",
    "while not done:\n",
    "    env.render()\n",
    "    state = torch.as_tensor(np.array(state, dtype=np.float32), device=device)\n",
    "    state_in = torch.unsqueeze(state, dim=0)\n",
    "    action = select_epsilon_greedy_action(state_in, epsilon=0.01)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    ep_rew += reward\n",
    "print(f'Total Return: {ep_rew}')\n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMe6VC57sR-W"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
