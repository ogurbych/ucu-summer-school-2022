{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMG8qMK9OY6r"
   },
   "source": [
    "# Install Dependecies to Render OpenAI Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvvBAoQVJsuU"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt-get update\n",
    "!apt-get install -y xvfb python-opengl ffmpeg\n",
    "!pip install pyglet==1.3.2\n",
    "!pip install gym pyvirtualdisplay\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ob-AUEpDO8MR"
   },
   "source": [
    "# Build DQN Agent and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "rxOHU5VsU7JB",
    "outputId": "91d6ed00-f3c0-4966-d96a-a762d67b1cb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of state features: 4\n",
      "Number of possible actions: 2\n"
     ]
    }
   ],
   "source": [
    "# Load gym environment and get action and state spaces.\n",
    "env = gym.make('CartPole-v0')\n",
    "num_features = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "print('Number of state features: {}'.format(num_features))\n",
    "print('Number of possible actions: {}'.format(num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q142f20ClXcy"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBjdd9KfFLml"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Dense neural network class.\"\"\"\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.out = nn.Linear(32, num_actions)\n",
    "\n",
    "    def forward(self, states):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x = F.relu(self.fc1(states))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "main_nn = DQN(num_features, num_actions).to(device)\n",
    "target_nn = DQN(num_features, num_actions).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(main_nn.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N7OT489fkEXO"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"Experience replay buffer that samples uniformly.\"\"\"\n",
    "    def __init__(self, size, device=\"cpu\"):\n",
    "        \"\"\"Initializes the buffer.\"\"\"\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.device = device\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        idx = np.random.choice(len(self.buffer), num_samples)\n",
    "        for i in idx:\n",
    "            elem = self.buffer[i]\n",
    "            state, action, reward, next_state, done = elem\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(done)\n",
    "        states = torch.as_tensor(np.array(states), device=self.device)\n",
    "        actions = torch.as_tensor(np.array(actions), device=self.device)\n",
    "        rewards = torch.as_tensor(np.array(rewards, dtype=np.float32), \n",
    "                                  device=self.device)\n",
    "        next_states = torch.as_tensor(np.array(next_states), device=self.device)\n",
    "        dones = torch.as_tensor(np.array(dones, dtype=np.float32), device=self.device)\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-jC9-cgWPyu"
   },
   "outputs": [],
   "source": [
    "def select_epsilon_greedy_action(state, epsilon):\n",
    "    \"\"\"Take random action with probability epsilon, else take best action.\"\"\"\n",
    "    result = np.random.uniform()\n",
    "    if result < epsilon:\n",
    "        return env.action_space.sample() # Random action (left or right).\n",
    "    else:\n",
    "        qs = main_nn(state).cpu().data.numpy()\n",
    "        return np.argmax(qs) # Greedy action for state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxjof_YfRFaN"
   },
   "outputs": [],
   "source": [
    "def train_step(states, actions, rewards, next_states, dones):\n",
    "    \"\"\"Perform a training iteration on a batch of data sampled from the experience\n",
    "    replay buffer.\n",
    "    \"\"\"\n",
    "    # Calculate targets.\n",
    "    max_next_qs = target_nn(next_states).max(-1).values\n",
    "    target = rewards + (1.0 - dones) * discount * max_next_qs\n",
    "    qs = main_nn(states)\n",
    "    action_masks = F.one_hot(actions, num_actions)\n",
    "    masked_qs = (action_masks * qs).sum(dim=-1)\n",
    "    loss = loss_fn(masked_qs, target.detach())\n",
    "    #nn.utils.clip_grad_norm_(loss, max_norm=10)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dk4gbIkIAkq9"
   },
   "source": [
    "# Start running the DQN algorithm and see how the algorithm learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "HdBCQEGNcVnV",
    "outputId": "fa7655c4-8ffd-4ea8-899c-45d3eff50bce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/1000. Epsilon: 0.999. Reward in last 100 episodes: 10.00\n",
      "Episode 50/1000. Epsilon: 0.949. Reward in last 100 episodes: 23.22\n",
      "Episode 100/1000. Epsilon: 0.899. Reward in last 100 episodes: 22.47\n",
      "Episode 150/1000. Epsilon: 0.849. Reward in last 100 episodes: 20.84\n",
      "Episode 200/1000. Epsilon: 0.799. Reward in last 100 episodes: 18.73\n",
      "Episode 250/1000. Epsilon: 0.749. Reward in last 100 episodes: 18.16\n",
      "Episode 300/1000. Epsilon: 0.699. Reward in last 100 episodes: 18.89\n",
      "Episode 350/1000. Epsilon: 0.649. Reward in last 100 episodes: 18.35\n",
      "Episode 400/1000. Epsilon: 0.599. Reward in last 100 episodes: 19.61\n",
      "Episode 450/1000. Epsilon: 0.549. Reward in last 100 episodes: 24.19\n",
      "Episode 500/1000. Epsilon: 0.499. Reward in last 100 episodes: 27.34\n",
      "Episode 550/1000. Epsilon: 0.449. Reward in last 100 episodes: 33.92\n",
      "Episode 600/1000. Epsilon: 0.399. Reward in last 100 episodes: 53.72\n",
      "Episode 650/1000. Epsilon: 0.349. Reward in last 100 episodes: 94.52\n",
      "Episode 700/1000. Epsilon: 0.299. Reward in last 100 episodes: 157.14\n",
      "Episode 750/1000. Epsilon: 0.249. Reward in last 100 episodes: 195.23\n",
      "Episode 800/1000. Epsilon: 0.199. Reward in last 100 episodes: 196.55\n",
      "Episode 850/1000. Epsilon: 0.149. Reward in last 100 episodes: 195.97\n",
      "Episode 900/1000. Epsilon: 0.099. Reward in last 100 episodes: 198.41\n",
      "Episode 950/1000. Epsilon: 0.050. Reward in last 100 episodes: 199.75\n",
      "Episode 1000/1000. Epsilon: 0.050. Reward in last 100 episodes: 199.11\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters.\n",
    "num_episodes = 1000\n",
    "epsilon = 1.0\n",
    "batch_size = 32\n",
    "discount = 0.99\n",
    "buffer = ReplayBuffer(100000, device=device)\n",
    "cur_frame = 0\n",
    "\n",
    "# Start training. Play game once and then train with a batch.\n",
    "last_100_ep_rewards = []\n",
    "for episode in range(num_episodes+1):\n",
    "    state = env.reset().astype(np.float32)\n",
    "    ep_reward, done = 0, False\n",
    "    while not done:\n",
    "        state_in = torch.from_numpy(np.expand_dims(state, axis=0)).to(device)\n",
    "        action = select_epsilon_greedy_action(state_in, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = next_state.astype(np.float32)\n",
    "        ep_reward += reward\n",
    "        # Save to experience replay.\n",
    "        buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        cur_frame += 1\n",
    "        # Copy main_nn weights to target_nn.\n",
    "        if cur_frame % 2000 == 0:\n",
    "            target_nn.load_state_dict(main_nn.state_dict())\n",
    "    \n",
    "        # Train neural network.\n",
    "        if len(buffer) > batch_size:\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "            loss = train_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "    if episode < 950:\n",
    "        epsilon -= 0.001\n",
    "\n",
    "    if len(last_100_ep_rewards) == 100:\n",
    "        last_100_ep_rewards = last_100_ep_rewards[1:]\n",
    "    last_100_ep_rewards.append(ep_reward)\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print(f'Episode {episode}/{num_episodes}. Epsilon: {epsilon:.3f}.'\n",
    "              f' Reward in last 100 episodes: {np.mean(last_100_ep_rewards):.2f}')\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxzxGYh_OmQg"
   },
   "source": [
    "# Display Result of Trained DQN Agent on Cartpole Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8sEgVgvM2OQ"
   },
   "outputs": [],
   "source": [
    "def show_video():\n",
    "    \"\"\"Enables video recording of gym environment and shows it.\"\"\"\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                                            loop controls style=\"height: 400px;\">\n",
    "                                            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                                            </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Video not found\")\n",
    "\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "2XelFhSJGWGX",
    "outputId": "74998b46-b1df-4c76-87f3-6e98b3eb7a30"
   },
   "outputs": [],
   "source": [
    "env = wrap_env(gym.make('CartPole-v0'))\n",
    "state = env.reset()\n",
    "done = False\n",
    "ep_rew = 0\n",
    "while not done:\n",
    "    env.render()\n",
    "    state = state.astype(np.float32)\n",
    "    state = torch.from_numpy(np.expand_dims(state, axis=0)).to(device)\n",
    "    action = select_epsilon_greedy_action(state, epsilon=0.01)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    ep_rew += reward\n",
    "print('Return on this episode: {}'.format(ep_rew))\n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCN_31YJtekB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
